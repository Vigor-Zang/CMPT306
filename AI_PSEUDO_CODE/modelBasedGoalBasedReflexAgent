This is a variation on a standard model based


The format of a goal based model based reflex agent

KEY DIFFERENCES FROM A simpleReflexAgent:
Knows what the world is like now:
	The agent has a state, meaning it has a persistent current conception of the world
		This conception is updated based of which sensors the agent has     // In our case, the sensors would be the types of info that it can register
	The agent known what its actions do to the world
**Such an agent also knows what it is trying to achieve and chooses an action which will(in a deterministic environment, not really the case in our game) lead to achieving its goals 	
Uses its state to then decide what action should be performed

function MODEL-BASED-RELFEX-AGENT(percept) returns an action
persistent: rules, a set of condition-action rules
	    model, a description of how the next state depends on current state and action
	    state, the agent's current perception of the world
	    action, the most recent action performed

state <-- UPDATE-STATE(state,action,percept,model)	 // The agent starts with an update of it's perception of the world
rule <-- RULE-MATCH(state,rules)  	 //   (THIS IS ESSENTIALLY THE CONDITIONAL NEST ALGORITHM EXCEPT THE RULES DEPEND ON THE STATE )
action <-- rule.Action
return action




NOTES:
In AI a modern approach these are the chapters associated with finding action sequences that achieve goals:  SEARCH (3), PLANNING(10+11)
